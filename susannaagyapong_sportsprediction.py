# -*- coding: utf-8 -*-
"""SusannaAgyapong_SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xg2Y3h23sB6ULHpA6x1zRA2qfsLs-2r6
"""

import pandas as pd

from sklearn.impute import SimpleImputer
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score

from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split
from google.colab import drive
drive.mount('/content/drive')

#Loading the dataset given
df2 = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data/players_22.csv") #player_22
df= pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data/male_players (legacy).csv") #male_legacy

df.head()

df.info()

df.dtypes

#drop things that be useless
df.columns

df.drop(columns = ['player_id', 'player_url', 'fifa_version', 'fifa_update', 'fifa_update_date', 'player_face_url', 'short_name', 'long_name'], inplace = True)

df.info()

y = df['overall'] #our target variable

quant = df.select_dtypes(include = [np.number]) #we get all the quantitative features

corr_matrix = quant.corr()

corr_matrix['overall'].sort_values(ascending = False)

#we are dropping columns that have a correlation less than 0.4 and greater than -0.4
for column in quant.columns:
  if corr_matrix['overall'][column] < 0.4 and corr_matrix['overall'][column] > -0.4:
    quant.drop(column, axis = 1, inplace = True)

quant.info()

corr_matrix = quant.corr()

corr_matrix['overall'].sort_values(ascending = False)

#we need to impute cause there are missing values
imputer = SimpleImputer(strategy = 'median')
q_columns = quant.columns
quant = imputer.fit_transform(quant)
quant = pd.DataFrame(quant, columns = q_columns)

quant

#after imputing then we need to scale
scaler = StandardScaler()
quant = pd.DataFrame(scaler.fit_transform(quant), columns = quant.columns)
quant

quant.drop('overall', axis = 1, inplace = True)

"""**Done Processing quantitative data!**

**Processing categorical data**
"""

cat = df.select_dtypes(include = ['object'])
cat.info()

important_cols = ['preferred_foot', 'body_type']
cat.drop(columns = [column for column in cat.columns if column not in important_cols], inplace =True)

cat.info()

#one hot encoding
cat = pd.get_dummies(cat).astype(int)
cat

"""**Done Processing Categorical data**

**Bring everything together and train and test the model**
"""

X = pd.concat([cat, quant], axis = 1)

Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size = 0.2, random_state = 42)

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

lin_reg = LinearRegression()

lin_reg.fit(Xtrain, Ytrain) #da training function
lin_reg_pred = lin_reg.predict(Xtest)

print(f""" Mean Aboslute Error={mean_absolute_error(lin_reg_pred,Ytest)}
          Mean Squared Error = {mean_squared_error(lin_reg_pred,Ytest)}
          Root Mean Squared error = {np.sqrt(mean_squared_error(lin_reg_pred,Ytest))}
          R2 score = {r2_score(lin_reg_pred,Ytest)}
""")

dTree = DecisionTreeRegressor()
dTree.fit(Xtrain, Ytrain)

dTree_pred = dTree.predict(Xtest)

print(f""" Mean Aboslute Error={mean_absolute_error(dTree_pred,Ytest)}
          Mean Squared Error = {mean_squared_error(dTree_pred,Ytest)}
          Root Mean Squared error = {np.sqrt(mean_squared_error(dTree_pred,Ytest))}
          R2 score = {r2_score(dTree_pred,Ytest)}
""")

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV

model = RandomForestRegressor()

parameter = {
    'n_estimators': [100,200,300],
    'max_depth': [5,7,9]
}

rand_search = RandomizedSearchCV(model, parameter, scoring = 'r2', cv = 3)

rand_pred= rand_search.fit(Xtrain, Ytrain)

best_params = rand_pred.best_params_
print(f"Best Parameters: {best_params}")

# Best estimator
best_estimator = rand_pred.best_estimator_
print(f"Best Estimator: {best_estimator}")

# Best score
best_score = rand_pred.best_score_
print(f"Best Cross-Validation Score: {best_score}")

df2.head()

df2.info()

df2.dtypes

df2.columns

df2.drop(columns = ['sofifa_id', 'player_url', 'club_logo_url', 'club_flag_url', 'player_face_url', 'short_name', 'long_name', 'nation_flag_url', 'nation_logo_url', ], inplace = True)

df2.columns

df2.info()

y = df['overall']

quant = df.select_dtypes(include = [np.number])

quant

df2.drop(columns = ['mentality_composure','goalkeeping_speed'], inplace = True)

corr_matrix = quant.corr()

corr_matrix['overall'].sort_values(ascending = False)

for column in quant.columns:
  if corr_matrix['overall'][column] < 0.6 and corr_matrix['overall'][column] > -0.6:
    quant.drop(column, axis = 1, inplace = True)

imputer = SimpleImputer(strategy = 'median')
q_columns = quant.columns
quant = imputer.fit_transform(quant)
quant = pd.DataFrame(quant, columns = q_columns)

quant

scaler = StandardScaler()
quant = pd.DataFrame(scaler.fit_transform(quant), columns = quant.columns)
quant

quant.drop('overall', axis = 1, inplace = True)

cat = df2.select_dtypes(include = ['object'])
cat.info()

important_cols = ['preferred_foot', 'body_type','work_rate']
cat.drop(columns = [column for column in cat.columns if column not in important_cols], inplace =True)
important_cols

cat.info()

#one hot encoding
cat = pd.get_dummies(cat).astype(int)
cat

X = pd.concat([cat, quant], axis = 1)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score

base_classifier = DecisionTreeClassifier()

bagging_classifier = BaggingClassifier(base_estimator=base_classifier, n_estimators=10, random_state=42)

bagging_classifier.fit(X_train, y_train)

y_pred = bagging_classifier.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print("Accuracy:", accuracy)

from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import ElasticNet, Lasso,LinearRegression,RidgeCV
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns

kf = KFold(n_splits=5,random_state=48,shuffle=True)
n=0

model2 = ElasticNet(alpha=0.00001)
    model2.fit(X_tr,y_tr)
    pred2[test_idx] = model2.predict(X_val)
    test2 += model2.predict(test[columns])/kf.n_splits
    rmse2 = mean_squared_error(y_val, model2.predict(X_val), squared=False)
    print(": model2 rmse = {}".format(rmse2))

model3 = xgb.XGBRegressor(**params_xgb)
    model3.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=200,verbose=False)
    pred3[test_idx] = model3.predict(X_val)
    test3 += model3.predict(test[columns])/kf.n_splits
    rmse3 = mean_squared_error(y_val, model3.predict(X_val), squared=False)
    print(": model4 rmse = {}".format(rmse4))